{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**last Changes made**\n\n1. Added a Classification Report.\n2. Added some text explaining the terms Precison, Recall and F1 score.\n\n\n***\n","metadata":{"_uuid":"2852ee80cbebe70f1acb26561531ca5b91f490e4"}},{"cell_type":"markdown","source":"****HAM1000 - Human Against Machine with 10000 training images ****\n> The HAM10000 Dataset: A Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions<br>\nhttps://arxiv.org/abs/1803.10417 <br>\nDataset can be downloaded from <br>\nhttps://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T <br>\nhere i imported it from Kaggle<br>\nhttps://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000/code\n\n* Publicly available - courtesy of Harvard<br>\n* Contains 10,015 dermatoscopic images\n* Also contains a metadata file (csv) with demographic information of each lesion.\n* More than 50% of lesions are confirmed through histopathology\n* the ground truth for the rest of the cases is either \n    * follow-up examination(follow-up) or \n    * expert concensus(consensus) or\n    * confirmation by in-vivo confocal microscopy(confocal)","metadata":{"_uuid":"d6f77793e7ebbcd523899bafaa9dae9763b9a0a5"}},{"cell_type":"code","source":"!pip install keras==2.15.0\nimport keras\n\nimport pandas as pd\nimport numpy as np\nimport os\n\nfrom numpy.random import seed\n\nnp.random.seed(42)\n\n\nimport tensorflow\nfrom tensorflow import keras\nimport numpy as np\ntensorflow.config.experimental.list_physical_devices()\n\n\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\n\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint('tensorflow version:')\nprint(tensorflow.__version__) #2.15.0\n#print(keras.__version__) #Successfully installed keras-3.3.3\n# Keras is now completely integrated with tensorflow API. Now keras is not a separate API standalone.","metadata":{"_uuid":"371731306c3e504b191979706e826c247def88dc","execution":{"iopub.status.busy":"2024-05-11T13:14:08.539938Z","iopub.execute_input":"2024-05-11T13:14:08.540698Z","iopub.status.idle":"2024-05-11T13:14:36.029886Z","shell.execute_reply.started":"2024-05-11T13:14:08.540660Z","shell.execute_reply":"2024-05-11T13:14:36.028778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LABELS**<br>\n\nExcerpts from the paper:<br>\n> The HAM10000 Dataset: A Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions<br>\nhttps://arxiv.org/abs/1803.10417\n\n\n\n **nv**<br>\n Melanocytic nevi are benign neoplasms of melanocytes and appear in a myriad of variants, which all are included in our series. The variants may differ significantly from a dermatoscopic point of view.<br>\n *[6705 images]*\n \n **mel**<br>\n Melanoma is a malignant neoplasm derived from melanocytes that may appear in different variants. **If excised in an early stage it can be cured by simple surgical excision.** Melanomas can be invasive or non-invasive (in situ). We included all variants of melanoma including melanoma in situ, but did exclude non-pigmented, subungual, ocular or mucosal melanoma.<br>*[1113 images]*\n \n \n**bkl**<br>\n \"Benign keratosis\" is a generic class that includes seborrheic ker- atoses (\"senile wart\"), solar lentigo - which can be regarded a flat variant of seborrheic keratosis - and lichen-planus like keratoses (LPLK), which corresponds to a seborrheic keratosis or a solar lentigo with inflammation\nand regression [22]. The three subgroups may look different dermatoscop- ically, but we grouped them together because they are similar biologically and often reported under the same generic term histopathologically. From a dermatoscopic view, lichen planus-like keratoses are especially challeng- ing because they can show morphologic features mimicking melanoma [23] and are often biopsied or excised for diagnostic reasons.<br>\n*[1099 images]*\n\n**bcc**<br>\nBasal cell carcinoma is a common variant of epithelial skin cancer that rarely metastasizes but grows destructively if untreated. It appears in different morphologic variants (flat, nodular, pigmented, cystic, etc) [21], which are all included in this set.<br>\n*[514 images]*\n \n**akiec**<br>\nActinic Keratoses (Solar Keratoses) and intraepithelial Carcinoma (Bowen’s disease) are common non-invasive, variants of squamous cell car- cinoma that can be treated locally without surgery. Some authors regard them as precursors of squamous cell carcinomas and not as actual carci- nomas. There is, however, agreement that these lesions may progress to invasive squamous cell carcinoma - which is usually not pigmented. Both neoplasms commonly show surface scaling and commonly are devoid of pigment. Actinic keratoses are more common on the face and Bowen’s disease is more common on other body sites. Because both types are in- duced by UV-light the surrounding skin is usually typified by severe sun damaged except in cases of Bowen’s disease that are caused by human papilloma virus infection and not by UV. Pigmented variants exists for Bowen’s disease [19] and for actinic keratoses [20]. Both are included in this set.<br>*[327 images]*\n\n\n**vasc**<br>\nVascular skin lesions in the dataset range from cherry angiomas to angiokeratomas [25] and pyogenic granulomas [26]. Hemorrhage is also included in this category.<br>\n*[142 images]*\n\n**df**<br>\nDermatofibroma is a benign skin lesion regarded as either a benign proliferation or an inflammatory reaction to minimal trauma. It is brown often showing a central zone of fibrosis dermatoscopically [24].<br>*[115 images]*\n\n\n<br>*[Total images = 10015]*","metadata":{"_uuid":"d3f6843b78793e1c047ca6909a7449dc9bfc3f1c"}},{"cell_type":"code","source":"os.listdir('../input')","metadata":{"_uuid":"d5a0a200bfc57c5489eaa930255d9420a7d01c47","execution":{"iopub.status.busy":"2024-05-11T13:14:36.032964Z","iopub.execute_input":"2024-05-11T13:14:36.033465Z","iopub.status.idle":"2024-05-11T13:14:36.048831Z","shell.execute_reply.started":"2024-05-11T13:14:36.033440Z","shell.execute_reply":"2024-05-11T13:14:36.047184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DATASET ANALYSIS","metadata":{}},{"cell_type":"code","source":"df_data = pd.read_csv('../input/HAM10000_metadata.csv')\n\ndf_data.head()","metadata":{"_uuid":"268503398ef61904e05a2c0b0667d589f08a19a8","execution":{"iopub.status.busy":"2024-05-11T13:14:36.049989Z","iopub.execute_input":"2024-05-11T13:14:36.050700Z","iopub.status.idle":"2024-05-11T13:14:36.130296Z","shell.execute_reply.started":"2024-05-11T13:14:36.050668Z","shell.execute_reply":"2024-05-11T13:14:36.129267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vizualizing Dataset ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.subplots_adjust(left=0.125, bottom=1, right=0.9, top=2, hspace=0.2)\nplt.subplot(2,4,1)\nplt.title(\"AGE\",fontsize=15)\nplt.ylabel(\"Count\")\ndf_data['age'].value_counts().plot.bar()\n\nplt.subplot(2,4,2)\nplt.title(\"GENDER\",fontsize=15)\nplt.ylabel(\"Count\")\ndf_data['sex'].value_counts().plot.bar()\n\nplt.subplot(2,4,3)\nplt.title(\"localization\",fontsize=15)\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\ndf_data['localization'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:14:36.132932Z","iopub.execute_input":"2024-05-11T13:14:36.133280Z","iopub.status.idle":"2024-05-11T13:14:37.190697Z","shell.execute_reply.started":"2024-05-11T13:14:36.133245Z","shell.execute_reply":"2024-05-11T13:14:37.189790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Skin diseases are found to be maximum in people aged around 45. Minimum for 10 and below. We also observe that the probability of having skin disease increases with the increase in age.\n2. Skin diseases are more prominent in Men as compared to Women and other gender.\n3. Skin diseases are more visible on the \"back\" of the body and least on the \"acral surfaces\"(such as limbs, fingers, or ears).\n4. The most found disease among people is Melanocytic nevi(nv) while the least found is Dermatofibroma.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\ndf_data['dx'].value_counts().plot.pie(autopct=\"%1.1f%%\")\nplt.subplot(1,2,2)\ndf_data['dx_type'].value_counts().plot.pie(autopct=\"%1.1f%%\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:14:37.191728Z","iopub.execute_input":"2024-05-11T13:14:37.191995Z","iopub.status.idle":"2024-05-11T13:14:37.531762Z","shell.execute_reply.started":"2024-05-11T13:14:37.191973Z","shell.execute_reply":"2024-05-11T13:14:37.530880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can see Dataset is heavily imbalanced","metadata":{}},{"cell_type":"markdown","source":"# BIVARIATE ANALYSIS","metadata":{}},{"cell_type":"code","source":"import seaborn as sns  # Import the seaborn library\n\nplt.figure(figsize=(25,10))\nplt.title('LOCALIZATION VS GENDER',fontsize = 15)\nsns.countplot(y='localization', hue='sex',data=df_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:14:37.532962Z","iopub.execute_input":"2024-05-11T13:14:37.533217Z","iopub.status.idle":"2024-05-11T13:14:38.281496Z","shell.execute_reply.started":"2024-05-11T13:14:37.533195Z","shell.execute_reply":"2024-05-11T13:14:38.280552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating Train and Val Sets","metadata":{}},{"cell_type":"markdown","source":"> ### Now Creating a directory structure\n\nIn these folders we will store the images that will later be fed to the Keras generators. ","metadata":{}},{"cell_type":"code","source":"# Creating a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATING FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 7 folders inside 'base_dir':\n\n# train_dir\n    # nv\n    # mel\n    # bkl\n    # bcc\n    # akiec\n    # vasc\n    # df\n \n# val_dir\n    # nv\n    # mel\n    # bkl\n    # bcc\n    # akiec\n    # vasc\n    # df\n\n# creating a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATING FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nnv = os.path.join(train_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(train_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(train_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(train_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(train_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(train_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(train_dir, 'df')\nos.mkdir(df)\n\n\n\n# creating new folders inside val_dir\nnv = os.path.join(val_dir, 'nv')\nos.mkdir(nv)\nmel = os.path.join(val_dir, 'mel')\nos.mkdir(mel)\nbkl = os.path.join(val_dir, 'bkl')\nos.mkdir(bkl)\nbcc = os.path.join(val_dir, 'bcc')\nos.mkdir(bcc)\nakiec = os.path.join(val_dir, 'akiec')\nos.mkdir(akiec)\nvasc = os.path.join(val_dir, 'vasc')\nos.mkdir(vasc)\ndf = os.path.join(val_dir, 'df')\nos.mkdir(df)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:14:38.282826Z","iopub.execute_input":"2024-05-11T13:14:38.283229Z","iopub.status.idle":"2024-05-11T13:14:38.296659Z","shell.execute_reply.started":"2024-05-11T13:14:38.283193Z","shell.execute_reply":"2024-05-11T13:14:38.295801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this will tell us how many images are associated with each lesion_id\ndf = df_data.groupby('lesion_id').count()\n\n# now we filter out lesion_id's that have only one image associated with it\ndf = df[df['image_id'] == 1]\n\ndf.reset_index(inplace=True)\n\ndf.head()","metadata":{"_uuid":"53e4b7b152ed831a7d7516156ac300c0e6985ffc","execution":{"iopub.status.busy":"2024-05-11T13:14:38.297994Z","iopub.execute_input":"2024-05-11T13:14:38.298278Z","iopub.status.idle":"2024-05-11T13:14:38.336754Z","shell.execute_reply.started":"2024-05-11T13:14:38.298255Z","shell.execute_reply":"2024-05-11T13:14:38.335976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here we identify lesion_id's that have duplicate images and those that have only\n# one image.\n\ndef identify_duplicates(x):\n    \n    unique_list = list(df['lesion_id'])\n    \n    if x in unique_list:\n        return 'no_duplicates'\n    else:\n        return 'has_duplicates'\n    \n# create a new colum that is a copy of the lesion_id column\ndf_data['duplicates'] = df_data['lesion_id']\n# apply the function to this new column\ndf_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n\ndf_data.head()","metadata":{"_uuid":"24720fe3ea9f2f4b571abd09ecfbb931d6429852","execution":{"iopub.status.busy":"2024-05-11T13:14:38.337756Z","iopub.execute_input":"2024-05-11T13:14:38.337997Z","iopub.status.idle":"2024-05-11T13:14:45.436683Z","shell.execute_reply.started":"2024-05-11T13:14:38.337976Z","shell.execute_reply":"2024-05-11T13:14:45.435759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data['duplicates'].value_counts()","metadata":{"_uuid":"08b7eef3e0ac4112f63b8fb26ce19d55483cbc04","execution":{"iopub.status.busy":"2024-05-11T13:14:45.441188Z","iopub.execute_input":"2024-05-11T13:14:45.441536Z","iopub.status.idle":"2024-05-11T13:14:45.449578Z","shell.execute_reply.started":"2024-05-11T13:14:45.441513Z","shell.execute_reply":"2024-05-11T13:14:45.448683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we filter out images that don't have duplicates\ndf = df_data[df_data['duplicates'] == 'no_duplicates']\n\ndf.shape","metadata":{"_uuid":"995445dfda2745165a53e61f42615104b951d4af","execution":{"iopub.status.busy":"2024-05-11T13:14:45.450857Z","iopub.execute_input":"2024-05-11T13:14:45.451181Z","iopub.status.idle":"2024-05-11T13:14:45.465663Z","shell.execute_reply.started":"2024-05-11T13:14:45.451152Z","shell.execute_reply":"2024-05-11T13:14:45.464822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### creating a stratified validation set \nso dividing a dataset into subgroups or strata based on label(dx)\n\nI choosed to split the train set in two parts : a small fraction (17% - These is just random number) became the validation set which the model is evaluated and the rest (83%) is used to train the model. ","metadata":{}},{"cell_type":"code","source":"# now we create a val set using df because we are sure that none of these images\n# have augmented duplicates in the train set\ny = df['dx']\n\n_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n\ndf_val.shape","metadata":{"_uuid":"39fde25b59a9452cf700c5b2ff82cc7cc45c4a33","execution":{"iopub.status.busy":"2024-05-11T13:14:45.466606Z","iopub.execute_input":"2024-05-11T13:14:45.466910Z","iopub.status.idle":"2024-05-11T13:14:45.484378Z","shell.execute_reply.started":"2024-05-11T13:14:45.466882Z","shell.execute_reply":"2024-05-11T13:14:45.483455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val['dx'].value_counts()","metadata":{"_uuid":"1df37227f7ce993d054ed5b8480ee724696fc210","execution":{"iopub.status.busy":"2024-05-11T13:14:45.487202Z","iopub.execute_input":"2024-05-11T13:14:45.487877Z","iopub.status.idle":"2024-05-11T13:14:45.495177Z","shell.execute_reply.started":"2024-05-11T13:14:45.487846Z","shell.execute_reply":"2024-05-11T13:14:45.494319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a train set that excludes images that are in the val set","metadata":{"_uuid":"08c5e12fcef2da5f49267a6b82161b2c52c2b20a"}},{"cell_type":"code","source":"# This set will be df_data excluding all rows that are in the val set\n\n# This function identifies if an image is part of the train\n# or val set.\ndef identify_val_rows(x):\n    # create a list of all the lesion_id's in the val set\n    val_list = list(df_val['image_id'])\n    \n    if str(x) in val_list:\n        return 'val'\n    else:\n        return 'train'\n\n# identify train and val rows\n\n# create a new colum that is a copy of the image_id column\ndf_data['train_or_val'] = df_data['image_id']\n# apply the function to this new column\ndf_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n   \n# filter out train rows\ndf_train = df_data[df_data['train_or_val'] == 'train']\n\n\nprint(len(df_train))\nprint(len(df_val))","metadata":{"_uuid":"03715a6cf5aeb6430ee144a84eb10dde216c0fb9","execution":{"iopub.status.busy":"2024-05-11T13:14:45.496200Z","iopub.execute_input":"2024-05-11T13:14:45.496472Z","iopub.status.idle":"2024-05-11T13:14:46.932753Z","shell.execute_reply.started":"2024-05-11T13:14:45.496450Z","shell.execute_reply":"2024-05-11T13:14:46.931745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['dx'].value_counts()","metadata":{"_uuid":"4b976a9018b1bd2dc0522c68339c5861534a1571","execution":{"iopub.status.busy":"2024-05-11T13:14:46.934369Z","iopub.execute_input":"2024-05-11T13:14:46.934749Z","iopub.status.idle":"2024-05-11T13:14:46.943472Z","shell.execute_reply.started":"2024-05-11T13:14:46.934713Z","shell.execute_reply":"2024-05-11T13:14:46.942649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val['dx'].value_counts()","metadata":{"_uuid":"1581d5a3e86f9673ae175102112017e30229bc37","execution":{"iopub.status.busy":"2024-05-11T13:14:46.944591Z","iopub.execute_input":"2024-05-11T13:14:46.945061Z","iopub.status.idle":"2024-05-11T13:14:47.052541Z","shell.execute_reply.started":"2024-05-11T13:14:46.945036Z","shell.execute_reply":"2024-05-11T13:14:47.051529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transfer the Images into the Folders","metadata":{"_uuid":"8812ad87c4fa18d2d82497df42c3895c7f10bc39"}},{"cell_type":"code","source":"# Set the image_id as the index in df_data\ndf_data.set_index('image_id', inplace=True)","metadata":{"_uuid":"4acee2b7879762e50b52df118a9b691515fe7ac0","execution":{"iopub.status.busy":"2024-05-11T13:14:47.053718Z","iopub.execute_input":"2024-05-11T13:14:47.053999Z","iopub.status.idle":"2024-05-11T13:14:47.062748Z","shell.execute_reply.started":"2024-05-11T13:14:47.053975Z","shell.execute_reply":"2024-05-11T13:14:47.061930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a list of images in each of the two folders\nfolder_1 = os.listdir('../input/ham10000_images_part_1')\nfolder_2 = os.listdir('../input/ham10000_images_part_2')\n\n# Get a list of train and val images\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('../input/ham10000_images_part_1', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('../input/ham10000_images_part_2', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    fname = image + '.jpg'\n    label = df_data.loc[image,'dx']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('../input/ham10000_images_part_1', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('../input/ham10000_images_part_2', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        # copy the image from the source to the destination\n        shutil.copyfile(src, dst)\n        ","metadata":{"_uuid":"eca02fbf066c8124d0cb465295bbd2593f5f045a","execution":{"iopub.status.busy":"2024-05-11T13:14:47.063747Z","iopub.execute_input":"2024-05-11T13:14:47.063995Z","iopub.status.idle":"2024-05-11T13:16:17.589467Z","shell.execute_reply.started":"2024-05-11T13:14:47.063975Z","shell.execute_reply":"2024-05-11T13:16:17.588686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir/train_dir/nv')))\nprint(len(os.listdir('base_dir/train_dir/mel')))\nprint(len(os.listdir('base_dir/train_dir/bkl')))\nprint(len(os.listdir('base_dir/train_dir/bcc')))\nprint(len(os.listdir('base_dir/train_dir/akiec')))\nprint(len(os.listdir('base_dir/train_dir/vasc')))\nprint(len(os.listdir('base_dir/train_dir/df')))","metadata":{"_uuid":"5a4847c4cc799c23e57bf2531d92117cb95e1b07","execution":{"iopub.status.busy":"2024-05-11T13:16:17.590826Z","iopub.execute_input":"2024-05-11T13:16:17.591574Z","iopub.status.idle":"2024-05-11T13:16:17.603983Z","shell.execute_reply.started":"2024-05-11T13:16:17.591537Z","shell.execute_reply":"2024-05-11T13:16:17.603078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir/val_dir/nv')))\nprint(len(os.listdir('base_dir/val_dir/mel')))\nprint(len(os.listdir('base_dir/val_dir/bkl')))\nprint(len(os.listdir('base_dir/val_dir/bcc')))\nprint(len(os.listdir('base_dir/val_dir/akiec')))\nprint(len(os.listdir('base_dir/val_dir/vasc')))\nprint(len(os.listdir('base_dir/val_dir/df')))","metadata":{"_uuid":"fd05c08cbfa00418dc333f5b67d1ff6e98aa973e","execution":{"iopub.status.busy":"2024-05-11T13:16:17.605216Z","iopub.execute_input":"2024-05-11T13:16:17.605490Z","iopub.status.idle":"2024-05-11T13:16:17.616397Z","shell.execute_reply.started":"2024-05-11T13:16:17.605459Z","shell.execute_reply":"2024-05-11T13:16:17.615558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Copy the train images  into aug_dir\nAnother temp. directory used so as to <br>\n1.preserving original data <br>\n2.if augmentation result produced is not satisfactorily it will be easier to iterate and refine the augmentation process within the temporary directories without affecting the original dataset.","metadata":{"_uuid":"cebcb5242ff542efb03be5086bf3796bea70c591"}},{"cell_type":"code","source":"# note that we are not augmenting class 'nv'\nclass_list = ['mel','bkl','bcc','akiec','vasc','df']\n\nfor item in class_list:\n    \n    # We are creating temporary directories here because we delete these directories later\n    # create a base dir\n    aug_dir = 'aug_dir'\n    os.mkdir(aug_dir)\n    # create a sub dir within this base dir(aug_dir) to store images of the same class\n    img_dir = os.path.join(aug_dir, 'img_dir')\n    os.mkdir(img_dir)\n\n    # Choose a class\n    img_class = item\n\n    # list all images in that directory\n    img_list = os.listdir('base_dir/train_dir/' + img_class)\n\n    # Copy images from the class train dir to the img_dir e.g. class 'mel'\n    for fname in img_list:\n            # source path to image\n            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n            # destination path to image\n            dst = os.path.join(img_dir, fname)\n            # copy the image from the source to the destination\n            shutil.copyfile(src, dst)\n\n\n    # point to a dir containing the images and not to the images themselves\n    path = aug_dir\n    save_path = 'base_dir/train_dir/' + img_class\n\n    # Create a data generator\n    datagen = ImageDataGenerator(\n        rotation_range=180,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        vertical_flip=True,\n        fill_mode='nearest')\n\n    batch_size = 50\n\n    aug_datagen = datagen.flow_from_directory(path,\n                                           save_to_dir=save_path,\n                                           save_format='jpg',\n                                                    target_size=(224,224),\n                                                    batch_size=batch_size)\n\n\n\n    # Generate the augmented images and add them to the training folders\n    \n    ###########\n    \n    num_aug_images_wanted = 6000 # total number of images we want to have in each class\n    \n    ###########\n    \n    num_files = len(os.listdir(img_dir))\n    num_batches = int(np.ceil((num_aug_images_wanted-num_files)/batch_size))\n\n    # run the generator and create about 6000 augmented images\n    for i in range(0,num_batches):\n\n        imgs, labels = next(aug_datagen)\n        \n    # delete temporary directory with the raw image files\n    shutil.rmtree('aug_dir')","metadata":{"_uuid":"8fe970d74e9d5a284420af4ad37d8aae89dc1c15","execution":{"iopub.status.busy":"2024-05-11T13:16:17.617614Z","iopub.execute_input":"2024-05-11T13:16:17.617902Z","iopub.status.idle":"2024-05-11T13:23:54.042944Z","shell.execute_reply.started":"2024-05-11T13:16:17.617868Z","shell.execute_reply":"2024-05-11T13:23:54.041918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many train images we now have in each folder.\n# This is the original images plus the augmented images.\n\nprint(len(os.listdir('base_dir/train_dir/nv')))\nprint(len(os.listdir('base_dir/train_dir/mel')))\nprint(len(os.listdir('base_dir/train_dir/bkl')))\nprint(len(os.listdir('base_dir/train_dir/bcc')))\nprint(len(os.listdir('base_dir/train_dir/akiec')))\nprint(len(os.listdir('base_dir/train_dir/vasc')))\nprint(len(os.listdir('base_dir/train_dir/df')))","metadata":{"_uuid":"b9bbc56bd25441150d2430dca2b07d8ebae57d95","execution":{"iopub.status.busy":"2024-05-11T13:23:54.044167Z","iopub.execute_input":"2024-05-11T13:23:54.044439Z","iopub.status.idle":"2024-05-11T13:23:54.072904Z","shell.execute_reply.started":"2024-05-11T13:23:54.044414Z","shell.execute_reply":"2024-05-11T13:23:54.072027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check how many val images we have in each folder.\n\nprint(len(os.listdir('base_dir/val_dir/nv')))\nprint(len(os.listdir('base_dir/val_dir/mel')))\nprint(len(os.listdir('base_dir/val_dir/bkl')))\nprint(len(os.listdir('base_dir/val_dir/bcc')))\nprint(len(os.listdir('base_dir/val_dir/akiec')))\nprint(len(os.listdir('base_dir/val_dir/vasc')))\nprint(len(os.listdir('base_dir/val_dir/df')))","metadata":{"_uuid":"21de03bdc63ecf78cc061d364d14d3216a544b43","execution":{"iopub.status.busy":"2024-05-11T13:23:54.073899Z","iopub.execute_input":"2024-05-11T13:23:54.074163Z","iopub.status.idle":"2024-05-11T13:23:54.082082Z","shell.execute_reply.started":"2024-05-11T13:23:54.074140Z","shell.execute_reply":"2024-05-11T13:23:54.081172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize 50 augmented images","metadata":{"_uuid":"767cb7d35e301369f020cdbb705da1620ba8e594"}},{"cell_type":"code","source":" \"\"\"\n    Ploting a grid of images.\n    Args:\n        ims (list of arrays): List of images to plot.\n        figsize (tuple): Size of the figure (width, height).\n        rows (int): Number of rows in the grid.\n        titles (list): List of titles for each image.\n        interp (bool): Whether to use interpolation in imshow.\n    \"\"\"\ndef plots(ims, figsize=(12,6), rows=5, interp=False, titles=None): # 12,6\n    #if first element of ims is a NumPy array\n    if type(ims[0]) is np.ndarray:  \n        #converting list of images into a NumPy array of data type uint8\n        ims = np.array(ims).astype(np.uint8)  \n        # if the last dimension of the array is 3 (indicating RGB channels) \n        if (ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)  # creates a new figure with the specified figsize.\n    #calculating the number of columns needed in the plot-grid based on the number of images and number of rows \n    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], interpolation=None if interp else 'none')\n        \n        \nplots(imgs, titles=None) # titles=labels will display the image labels","metadata":{"_uuid":"5f0e13a8455af926fe449e1b3ea818b704724202","execution":{"iopub.status.busy":"2024-05-11T13:23:54.083195Z","iopub.execute_input":"2024-05-11T13:23:54.083449Z","iopub.status.idle":"2024-05-11T13:23:55.844511Z","shell.execute_reply.started":"2024-05-11T13:23:54.083426Z","shell.execute_reply":"2024-05-11T13:23:55.843604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# End of Data Preparation\n### ===================================================================================== ###\n# Start of Model Building","metadata":{"_uuid":"c3e2126a39c06568a1f95da2ab42353447d1be20","execution":{"iopub.status.busy":"2024-05-11T13:23:55.845619Z","iopub.execute_input":"2024-05-11T13:23:55.845924Z","iopub.status.idle":"2024-05-11T13:23:55.849771Z","shell.execute_reply.started":"2024-05-11T13:23:55.845898Z","shell.execute_reply":"2024-05-11T13:23:55.848888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#Can also load pretrained networks such as mobilenet or VGG16<br>","metadata":{}},{"cell_type":"markdown","source":"## The CNN model is a repeated network of the following layers:\n* Convolutional\n* Pooling\n* Dropout\n* Flatten\n* Dense\n\n**Optimizer**: Adam\n\n**Activation function** used: Softmax","metadata":{}},{"cell_type":"markdown","source":"### Using MobileNet CNN Model","metadata":{"_uuid":"8ee4ee41f1b16083bd9fc20ee9dec40acccc97dd"}},{"cell_type":"code","source":"image_size=224\nimg_shape = (image_size, image_size, 3)\n\nprint('Loading MobileNetV2 ...')\nmobile = tensorflow.keras.applications.MobileNetV2(input_shape=img_shape, weights=\"imagenet\")\n\n\nprint('MobileNetV2 loaded')\n\nmobile.trainable = False  # Freeze pre-trained layers\n    \n#base_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:23:55.856351Z","iopub.execute_input":"2024-05-11T13:23:55.856659Z","iopub.status.idle":"2024-05-11T13:23:58.362300Z","shell.execute_reply.started":"2024-05-11T13:23:55.856613Z","shell.execute_reply":"2024-05-11T13:23:58.361216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mobile.summary()","metadata":{"_uuid":"960449ec7ecdda92ba733ad23b00b7be605f3d4b","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-11T13:23:58.363404Z","iopub.execute_input":"2024-05-11T13:23:58.363700Z","iopub.status.idle":"2024-05-11T13:23:58.789194Z","shell.execute_reply.started":"2024-05-11T13:23:58.363675Z","shell.execute_reply":"2024-05-11T13:23:58.769287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(mobile.layers)","metadata":{"_uuid":"5b7922bdf625675834d9b63ec0e85351bd9f3c0f","execution":{"iopub.status.busy":"2024-05-11T13:23:58.790416Z","iopub.execute_input":"2024-05-11T13:23:58.790756Z","iopub.status.idle":"2024-05-11T13:23:58.798221Z","shell.execute_reply.started":"2024-05-11T13:23:58.790726Z","shell.execute_reply":"2024-05-11T13:23:58.797116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many layers does MobileNetV2 have?\nlen(mobile.layers)","metadata":{"_uuid":"f832e5865c65a013a06dbf5d500c0381020c56d5","execution":{"iopub.status.busy":"2024-05-11T13:23:58.799628Z","iopub.execute_input":"2024-05-11T13:23:58.800005Z","iopub.status.idle":"2024-05-11T13:23:58.809220Z","shell.execute_reply.started":"2024-05-11T13:23:58.799931Z","shell.execute_reply":"2024-05-11T13:23:58.808204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mobile.output_shape","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:23:58.810663Z","iopub.execute_input":"2024-05-11T13:23:58.811023Z","iopub.status.idle":"2024-05-11T13:23:58.821534Z","shell.execute_reply.started":"2024-05-11T13:23:58.810994Z","shell.execute_reply":"2024-05-11T13:23:58.820367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine tuning\nWe require fine tuning when we use **transfer learning** from a pre-trained model.<br>\nTransfer learning occurs when we use knowledge that was gained from solving one problem and then applying that into new related problem.<br>\nAs i am using MobilenetV2 which is a transfer learning model.<br>\nFine tuning is a way/process to utilise transfer learning from a pre trained model, here we takes a model which is already been trained for a given task, and then tuning or tweaking that model to make it perform a second similar task.<br>\nthis is when original task is similar to original task.","metadata":{}},{"cell_type":"code","source":"# CREATE THE MODEL ARCHITECTURE\n\n# Exclude the last layers of the above model.\n# This will include all layers up to and including global_average_pooling2d_1\nx = mobile.layers[-2].output\n\n# Create a new dense layer for predictions\n# 7 corresponds to the number of classes\npredictions = Dense(7, activation='softmax')(x)   # will store probability of 7 classes.\n\n# inputs=mobile.input selects the input layer, outputs=predictions refers to dense layer created above.\n\nmodel = Model(inputs=mobile.input, outputs=predictions)","metadata":{"_uuid":"4dd9dcf26d85a57a113e6b158cf8fceeca7f99de","execution":{"iopub.status.busy":"2024-05-11T13:23:58.823030Z","iopub.execute_input":"2024-05-11T13:23:58.823390Z","iopub.status.idle":"2024-05-11T13:23:58.865782Z","shell.execute_reply.started":"2024-05-11T13:23:58.823363Z","shell.execute_reply":"2024-05-11T13:23:58.864764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"_uuid":"b38734b72afc4289ab187a9e683cbda6bf3269bc","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-11T13:23:58.867032Z","iopub.execute_input":"2024-05-11T13:23:58.867333Z","iopub.status.idle":"2024-05-11T13:23:59.292570Z","shell.execute_reply.started":"2024-05-11T13:23:58.867307Z","shell.execute_reply":"2024-05-11T13:23:59.291699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mobile.trainable = True\n\n# Let's take a look to see how many layers are in the base model\nprint(f'Number of layers in the base model: {len(mobile.layers)}')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:23:59.293689Z","iopub.execute_input":"2024-05-11T13:23:59.293958Z","iopub.status.idle":"2024-05-11T13:23:59.306325Z","shell.execute_reply.started":"2024-05-11T13:23:59.293934Z","shell.execute_reply":"2024-05-11T13:23:59.305466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fine-tune from this layer onwards\nfine_tuning = 100\n\n# Freeze all the layers before fine_tuned_ind\nfor layer in mobile.layers[:fine_tuning]:\n    layer.trainable =  False","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:23:59.307412Z","iopub.execute_input":"2024-05-11T13:23:59.307693Z","iopub.status.idle":"2024-05-11T13:23:59.316374Z","shell.execute_reply.started":"2024-05-11T13:23:59.307666Z","shell.execute_reply":"2024-05-11T13:23:59.315390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set up generators\n> ### Applied Data augmentation using ImageDatagenerator before model training\nImageDataGenerator generates augmentation of images in real-time while the model is still training. One can apply any random transformations on each training image as it is passed to the model.","metadata":{}},{"cell_type":"code","source":"train_path = 'base_dir/train_dir'\nvalid_path = 'base_dir/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps =  np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:23:59.317681Z","iopub.execute_input":"2024-05-11T13:23:59.318264Z","iopub.status.idle":"2024-05-11T13:23:59.328957Z","shell.execute_reply.started":"2024-05-11T13:23:59.318233Z","shell.execute_reply":"2024-05-11T13:23:59.327986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    preprocessing_function= \\\n    tensorflow.keras.applications.mobilenet.preprocess_input)\n\ntrain_batches = datagen.flow_from_directory(train_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=train_batch_size)\n\nvalid_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=val_batch_size)\n\ntest_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=1)\n# by default shuffle parameter is set true","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:23:59.330176Z","iopub.execute_input":"2024-05-11T13:23:59.330433Z","iopub.status.idle":"2024-05-11T13:24:01.458341Z","shell.execute_reply.started":"2024-05-11T13:23:59.330411Z","shell.execute_reply":"2024-05-11T13:24:01.457596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the labels that are associated with each index\nprint(valid_batches.class_indices)","metadata":{"_uuid":"62e7a784a33d4c868f49a3ef1f9acbc7186e3338","execution":{"iopub.status.busy":"2024-05-11T13:24:01.459572Z","iopub.execute_input":"2024-05-11T13:24:01.460231Z","iopub.status.idle":"2024-05-11T13:24:01.464788Z","shell.execute_reply.started":"2024-05-11T13:24:01.460194Z","shell.execute_reply":"2024-05-11T13:24:01.463800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add weights to try to make the model more sensitive to melanoma\n\nclass_weights={\n    0: 1.0, # akiec\n    1: 1.0, # bcc\n    2: 1.0, # bkl\n    3: 1.0, # df\n    4: 3.0, # mel # Try to make the model more sensitive to Melanomas it is deadly.\n    5: 1.0, # nv\n    6: 1.0, # vasc\n}","metadata":{"_uuid":"3001857c9a3c2b15c2343627e340eb1ae858fae9","execution":{"iopub.status.busy":"2024-05-11T13:24:01.466023Z","iopub.execute_input":"2024-05-11T13:24:01.466318Z","iopub.status.idle":"2024-05-11T13:24:01.477978Z","shell.execute_reply.started":"2024-05-11T13:24:01.466296Z","shell.execute_reply":"2024-05-11T13:24:01.477103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the Model","metadata":{"_uuid":"13cf63a53e5195cb8a9725d2506c71108bc478b9"}},{"cell_type":"code","source":"# Define Top2 and Top3 Accuracy\n\nfrom tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n\ndef top_2_accuracy(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:24:01.479081Z","iopub.execute_input":"2024-05-11T13:24:01.479345Z","iopub.status.idle":"2024-05-11T13:24:01.488574Z","shell.execute_reply.started":"2024-05-11T13:24:01.479322Z","shell.execute_reply":"2024-05-11T13:24:01.487788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting Optimizer and Annealer\nOnce our layers are added to the model, we need to set up a score function, a loss function and an optimisation algorithm. We define the loss function to measure how poorly our model performs on images with known labels. It is the error rate between the oberved labels and the predicted ones. We use a specific form for categorical classifications (>2 classes) called the \"categorical_crossentropy\". The most important function is the optimizer. This function will iteratively improve parameters (filters kernel values, weights and bias of neurons ...) in order to minimise the loss. I choosed Adam optimizer because it combines the advantages of two other extensions of stochastic gradient descent. Specifically:\n\nAdaptive Gradient Algorithm (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n\nRoot Mean Square Propagation (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n\n**Adam** realizes the benefits of both AdaGrad and RMSProp.\nAdam is a popular algorithm in the field of deep learning because it achieves good results fast.\n\nThe metric function \"accuracy\" is used is to evaluate the performance our model. This metric function is similar to the loss function, except that the results from the metric evaluation are not used when training the model (only for evaluation).","metadata":{}},{"cell_type":"markdown","source":"* Saving the best performing model during training using **ModelCheckpoint** allows us to track progress and load the best model later.<br>\n* Using **ReduceLROnPlateau** helps adjust the learning rate during training to potentially avoid getting stuck in local minima.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nfilepath = \"model.keras\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_top_2_accuracy', verbose=1, \n                             save_best_only=True, mode='max',save_weights_only=True)\n\n# Setting up a learning rate annealer (learning_rate_reduction)\nreduce_lr = ReduceLROnPlateau(monitor='val_top_2_accuracy', factor=0.7, patience=3, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                             \n# early stop if not improvement of accuracy after 5 epochs\nearly = EarlyStopping(patience=6, verbose=1) \n\ncallbacks_list = [checkpoint, reduce_lr]\n\n\n\nfrom tensorflow.keras.optimizers import Adam\n# # Define the optimizer(Adam) and Compile the model\nmodel.compile(Adam(learning_rate=0.01), loss='categorical_crossentropy', \n              metrics=[categorical_accuracy, top_2_accuracy])\n","metadata":{"_uuid":"4a5e3bc3cf44f1d4326c34ad880a302ba082e9d5","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-11T13:24:01.489491Z","iopub.execute_input":"2024-05-11T13:24:01.489752Z","iopub.status.idle":"2024-05-11T13:24:01.520009Z","shell.execute_reply.started":"2024-05-11T13:24:01.489731Z","shell.execute_reply":"2024-05-11T13:24:01.519331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training occurs whenever we calls this fit() function\nhistory = model.fit(train_batches, steps_per_epoch=train_steps, \n                              class_weight=class_weights,\n                    validation_data=valid_batches,\n                    validation_steps=val_steps,\n                    epochs=30, verbose=1,\n                   callbacks=callbacks_list)\n# by default shuffle parameter is set true\n# epochs means our model is going to process or trained on all of the train dataset given number of times before completing total process.","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:24:01.521001Z","iopub.execute_input":"2024-05-11T13:24:01.521246Z","iopub.status.idle":"2024-05-11T13:42:51.011552Z","shell.execute_reply.started":"2024-05-11T13:24:01.521225Z","shell.execute_reply":"2024-05-11T13:42:51.010783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from keras.utils import plot_model\n#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:42:51.012919Z","iopub.execute_input":"2024-05-11T13:42:51.013205Z","iopub.status.idle":"2024-05-11T13:42:51.017241Z","shell.execute_reply.started":"2024-05-11T13:42:51.013181Z","shell.execute_reply":"2024-05-11T13:42:51.016227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model.keras')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:42:51.018257Z","iopub.execute_input":"2024-05-11T13:42:51.018489Z","iopub.status.idle":"2024-05-11T13:42:51.699940Z","shell.execute_reply.started":"2024-05-11T13:42:51.018469Z","shell.execute_reply":"2024-05-11T13:42:51.699101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate the model using the val set","metadata":{"_uuid":"c3e43e3f2943db4be9d75831fe23661ae9deb44b"}},{"cell_type":"code","source":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","metadata":{"_uuid":"710ee26097924153647ac432c8ade29383fe42f1","execution":{"iopub.status.busy":"2024-05-11T13:42:51.701019Z","iopub.execute_input":"2024-05-11T13:42:51.701280Z","iopub.status.idle":"2024-05-11T13:42:51.707797Z","shell.execute_reply.started":"2024-05-11T13:42:51.701257Z","shell.execute_reply":"2024-05-11T13:42:51.706720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here the the last epoch will be used.\n\nval_loss, val_cat_acc, val_top_2_acc = \\\nmodel.evaluate(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)","metadata":{"_uuid":"68603a5e8cb5e507db95074a07b552a61fa48e11","execution":{"iopub.status.busy":"2024-05-11T13:42:51.708868Z","iopub.execute_input":"2024-05-11T13:42:51.709171Z","iopub.status.idle":"2024-05-11T13:42:59.760595Z","shell.execute_reply.started":"2024-05-11T13:42:51.709147Z","shell.execute_reply":"2024-05-11T13:42:59.759805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here the best epoch will be used.\n\nmodel.load_weights('model.keras')\n\nval_loss, val_cat_acc, val_top_2_acc= \\\nmodel.evaluate(test_batches, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_cat_acc:', val_cat_acc)\nprint('val_top_2_acc:', val_top_2_acc)\n","metadata":{"_uuid":"897f066da922d81fefa165a6b911a741c52ef7f5","execution":{"iopub.status.busy":"2024-05-11T13:42:59.761868Z","iopub.execute_input":"2024-05-11T13:42:59.762191Z","iopub.status.idle":"2024-05-11T13:43:10.681430Z","shell.execute_reply.started":"2024-05-11T13:42:59.762166Z","shell.execute_reply":"2024-05-11T13:43:10.680462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot the Training Curves","metadata":{"_uuid":"c3fffba5e0aa9088cda1865c7b8d75d72c20d0f6"}},{"cell_type":"code","source":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['categorical_accuracy']\nval_acc = history.history['val_categorical_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_top2_acc = history.history['top_2_accuracy']\nval_top2_acc = history.history['val_top_2_accuracy']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training cat acc')\nplt.plot(epochs, val_acc, 'b', label='Validation cat acc')\nplt.title('Training and validation cat accuracy')\nplt.legend()\nplt.figure()\n\n\nplt.plot(epochs, train_top2_acc, 'bo', label='Training top2 acc')\nplt.plot(epochs, val_top2_acc, 'b', label='Validation top2 acc')\nplt.title('Training and validation top2 accuracy')\nplt.legend()\nplt.figure()\n\nplt.show()","metadata":{"_uuid":"0cbd11ef4286a751ef2918361af035d356f341ae","execution":{"iopub.status.busy":"2024-05-11T13:43:10.682813Z","iopub.execute_input":"2024-05-11T13:43:10.683106Z","iopub.status.idle":"2024-05-11T13:43:11.567612Z","shell.execute_reply.started":"2024-05-11T13:43:10.683081Z","shell.execute_reply":"2024-05-11T13:43:11.566678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation on the test dataset\n\nloss, acc = model.evaluate(test_batches,steps=len(df_val), verbose=0)[:2]\nprint(f'Test loss: {loss:.2f}')\nprint(f'Test accuracy: {acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:43:11.568718Z","iopub.execute_input":"2024-05-11T13:43:11.568993Z","iopub.status.idle":"2024-05-11T13:43:18.675025Z","shell.execute_reply.started":"2024-05-11T13:43:11.568967Z","shell.execute_reply":"2024-05-11T13:43:18.674064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create a Confusion Matrix","metadata":{"_uuid":"4204e4056c8d12c1fee72b97912879cad4ee483f"}},{"cell_type":"code","source":"# Get the labels of the test images.\n\ntest_labels = test_batches.classes","metadata":{"_uuid":"74a66905f7a2d702f3d2aad9abf9fe114b96f0ff","execution":{"iopub.status.busy":"2024-05-11T13:43:18.676017Z","iopub.execute_input":"2024-05-11T13:43:18.676274Z","iopub.status.idle":"2024-05-11T13:43:18.680303Z","shell.execute_reply.started":"2024-05-11T13:43:18.676252Z","shell.execute_reply":"2024-05-11T13:43:18.679406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need these to plot the confusion matrix.\ntest_labels","metadata":{"_uuid":"53f4b22617285e923f336cdb2ffcbe1f9ff5e5db","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-11T13:43:18.681569Z","iopub.execute_input":"2024-05-11T13:43:18.681866Z","iopub.status.idle":"2024-05-11T13:43:18.698818Z","shell.execute_reply.started":"2024-05-11T13:43:18.681842Z","shell.execute_reply":"2024-05-11T13:43:18.697975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the label associated with each class\ntest_batches.class_indices","metadata":{"_uuid":"d5113e039e8384b96595751e084f0c5ed677080a","execution":{"iopub.status.busy":"2024-05-11T13:43:18.699913Z","iopub.execute_input":"2024-05-11T13:43:18.700232Z","iopub.status.idle":"2024-05-11T13:43:18.712049Z","shell.execute_reply.started":"2024-05-11T13:43:18.700202Z","shell.execute_reply":"2024-05-11T13:43:18.711210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a prediction\npredictions = model.predict(test_batches, steps=len(df_val), verbose=1)","metadata":{"_uuid":"701dafc5874aa60a054a74c04170cb7e8d750e94","execution":{"iopub.status.busy":"2024-05-11T13:43:18.713095Z","iopub.execute_input":"2024-05-11T13:43:18.713370Z","iopub.status.idle":"2024-05-11T13:43:27.486245Z","shell.execute_reply.started":"2024-05-11T13:43:18.713347Z","shell.execute_reply":"2024-05-11T13:43:27.485187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.shape","metadata":{"_uuid":"dcce17ac0488ff90d29b11592c9226ed1bb210fb","execution":{"iopub.status.busy":"2024-05-11T13:43:27.488571Z","iopub.execute_input":"2024-05-11T13:43:27.489413Z","iopub.status.idle":"2024-05-11T13:43:27.496583Z","shell.execute_reply.started":"2024-05-11T13:43:27.489366Z","shell.execute_reply":"2024-05-11T13:43:27.495691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\n\n","metadata":{"_uuid":"7cfd9bdbbd27e27d9c5de7c6593527686445ea89","execution":{"iopub.status.busy":"2024-05-11T13:43:27.497750Z","iopub.execute_input":"2024-05-11T13:43:27.498041Z","iopub.status.idle":"2024-05-11T13:43:27.507283Z","shell.execute_reply.started":"2024-05-11T13:43:27.498017Z","shell.execute_reply":"2024-05-11T13:43:27.506399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels.shape","metadata":{"_uuid":"8d499136cdb5fdf356515beb6e0cd1130ed584db","execution":{"iopub.status.busy":"2024-05-11T13:43:27.508385Z","iopub.execute_input":"2024-05-11T13:43:27.508671Z","iopub.status.idle":"2024-05-11T13:43:27.522919Z","shell.execute_reply.started":"2024-05-11T13:43:27.508624Z","shell.execute_reply":"2024-05-11T13:43:27.522060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","metadata":{"_uuid":"940b71bb2b37d847ba81dd67ca50c7fd5785fd35","execution":{"iopub.status.busy":"2024-05-11T13:43:27.524019Z","iopub.execute_input":"2024-05-11T13:43:27.524292Z","iopub.status.idle":"2024-05-11T13:43:27.534155Z","shell.execute_reply.started":"2024-05-11T13:43:27.524260Z","shell.execute_reply":"2024-05-11T13:43:27.533368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_batches.class_indices","metadata":{"_uuid":"97c6b493c368ff6565782c1bb15827f5d349ef79","execution":{"iopub.status.busy":"2024-05-11T13:43:27.536191Z","iopub.execute_input":"2024-05-11T13:43:27.536492Z","iopub.status.idle":"2024-05-11T13:43:27.545894Z","shell.execute_reply.started":"2024-05-11T13:43:27.536462Z","shell.execute_reply":"2024-05-11T13:43:27.544973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","metadata":{"_uuid":"0ddbd33a93468075c64ba49188a6d272a5c7828f","execution":{"iopub.status.busy":"2024-05-11T13:43:27.546937Z","iopub.execute_input":"2024-05-11T13:43:27.547250Z","iopub.status.idle":"2024-05-11T13:43:28.052787Z","shell.execute_reply.started":"2024-05-11T13:43:27.547220Z","shell.execute_reply":"2024-05-11T13:43:28.051950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate the Classification Report","metadata":{"_uuid":"571f3f23d58daa03ec3fbf9207363063b2a63373"}},{"cell_type":"code","source":"# Get the index of the class with the highest probability score\ny_pred = np.argmax(predictions, axis=1)\n\n# Get the labels of the test images.\ny_true = test_batches.classes","metadata":{"_uuid":"ebc31a49c8e8ab5d1305e46a7d638e0da326da8b","execution":{"iopub.status.busy":"2024-05-11T13:43:28.053823Z","iopub.execute_input":"2024-05-11T13:43:28.054092Z","iopub.status.idle":"2024-05-11T13:43:28.058432Z","shell.execute_reply.started":"2024-05-11T13:43:28.054069Z","shell.execute_reply":"2024-05-11T13:43:28.057531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n\nprint(report)","metadata":{"_uuid":"05027fe3475ab922340f9d905ec1239e2157fa05","execution":{"iopub.status.busy":"2024-05-11T13:43:28.059612Z","iopub.execute_input":"2024-05-11T13:43:28.060432Z","iopub.status.idle":"2024-05-11T13:43:28.079046Z","shell.execute_reply.started":"2024-05-11T13:43:28.060388Z","shell.execute_reply":"2024-05-11T13:43:28.078243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Recall** = Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.","metadata":{"_uuid":"d8313f0f4024fdb8053f943136880e63ac962ac2"}},{"cell_type":"code","source":"#PLot fractional incorrect misclassifications\nincorr_fraction = 1 - np.diag(cm) / np.sum(cm, axis=1)\nplt.bar(np.arange(7), incorr_fraction)\nplt.xlabel('True Label')\nplt.ylabel('Fraction of incorrect predictions')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:43:57.607295Z","iopub.execute_input":"2024-05-11T13:43:57.607865Z","iopub.status.idle":"2024-05-11T13:43:57.882445Z","shell.execute_reply.started":"2024-05-11T13:43:57.607835Z","shell.execute_reply":"2024-05-11T13:43:57.881574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# End of Model Building\n### ===================================================================================== ###\n","metadata":{"_uuid":"4c46f3f1d257241f96b4aac7eb96831ff8bbea33","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Delete the image data directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('base_dir')","metadata":{"_uuid":"f774cd15c6de188d4bb150f25ab600e5cbc06031","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Resources\n\nThese are some resources that I used:","metadata":{"_uuid":"4b6056bb27006ebc85fb54bbc8b9b989bd756ff1","trusted":true}},{"cell_type":"markdown","source":"1. Excellent tutorial series by deeplizard on how to use Mobilenet with Tensorflow.js<br>\nhttps://www.youtube.com/watch?v=HEQDRWMK6yY\n\n2. Tutorial by Minsuk Heo on Accuracy, Precision and F1 Score<br>\nhttps://www.youtube.com/watch?v=HBi-P5j0Kec\n\n3. Tutorial by Data School on how to evaluate a classifier<br>\nhttps://www.youtube.com/watch?v=85dtiMz9tSo\n\n3. Tensorflow.js gallery of projects<br>\nhttps://github.com/tensorflow/tfjs/blob/master/GALLERY.md\n\n","metadata":{"_uuid":"d5f5d88e7cda18fb86c7e9715e488536e4424673"}},{"cell_type":"markdown","source":"### Conclusion","metadata":{"_uuid":"55623033f552beec0101f2d8241c122404b1f82f"}},{"cell_type":"markdown","source":"Many thanks to Kevin Mader (@kmader) for posting this dataset. Thanks Kaggle for the free GPU.\n\nThank you for reading. ","metadata":{"_uuid":"8d99df295cd21dfd7f15176a35eabd1ed0d41cdb"}}]}